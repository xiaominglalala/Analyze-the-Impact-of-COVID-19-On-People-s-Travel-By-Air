<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>Project_4</title>
</head>
<body>
<h1 id='1-introduction'>1 Introduction</h1>
<p>Travel has been severely restricted since the COVID-19 outbreak began in late 2019, but is this the result of government restrictions, or people simply choosing not to go out because of scary of unknown COVID-19. So we wanted to use the data from Twitter, the number of COVID-19 diagnoses and new cases per day, as well as flights over the last three years, to find out how does the COVID-19 affects people&#39;s travel and their mood. Based on this original intention, the project completed the following tasks:</p>
<ul>
<li>Data Collecting</li>
<li>Data Pre-Processing</li>
<li>Data Analysis</li>
<li>Data Conclusion</li>
<li>Future Work</li>

</ul>
<h1 id='2-data-collecting'>2 Data Collecting</h1>
<blockquote><p>这部分基本可以直接用 project 1 的内容。</p>
<p>预计在网页上占一个 tab，首先是 overview 介绍这三个数据集的：</p>
<ul>
<li>采集来源：给个 reference 的 url</li>
<li>采集方式：爬取/api/下载</li>
<li>数据格式：每一列的含义、解释</li>
<li>数据目的：这个数据集有什么用(简单做个铺垫啥的)</li>

</ul>
<p>project1 做过数据集的 cleanliness/data issue，把这部分也加上</p>
<p>最后每个数据集用 plotly 等工具展示一个数据样本</p>
</blockquote>
<h2 id='21-covid-19-cases-and-death-dataset'>2.1 Covid-19 Cases and Death Dataset</h2>
<h3 id='211-overview'>2.1.1 Overview</h3>
<p>We use CDC api to collect <a href='https://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36'>Covid-19 Cases and Death</a> dataset. We need a APP TOKEN to request their api.</p>
<p>Requested data are in the form of json array. I transform them into CSV data with some adjustments (to make data more easy to look at). Their api has a paging limitation so I can&#39;t just get my data in a simple request. I made a loop to request 5000 records per request, and save them into CSV files.</p>
<h3 id='212-data-sample-and-definition'>2.1.2 Data Sample and Definition</h3>
<figure><table>
<thead>
<tr><th>Column Name</th><th>Description</th></tr></thead>
<tbody><tr><td>submission_date</td><td>Date of counts</td></tr><tr><td>state</td><td>Jurisdiction</td></tr><tr><td>tot_cases</td><td>Total number of cases</td></tr><tr><td>conf_cases</td><td>Total confirmed cases</td></tr><tr><td>prob_cases</td><td>Total probable cases</td></tr><tr><td>new_case</td><td>Number of new cases</td></tr><tr><td>pnew_case</td><td>Number of new probable cases</td></tr><tr><td>tot_death</td><td>Total number of deaths</td></tr><tr><td>conf_death</td><td>Total number of confirmed deaths</td></tr><tr><td>prob_death</td><td>Total number of probable deaths</td></tr><tr><td>new_death</td><td>Number of new deaths</td></tr><tr><td>pnew_death</td><td>Number of new probable deaths</td></tr><tr><td>created_at</td><td>Date and time record was created</td></tr><tr><td>consent_cases</td><td>If Agree, then confirmed and probable cases are included. If Not Agree, then only total cases are included.</td></tr><tr><td>consent_deaths</td><td>If Agree, then confirmed and probable deaths are included. If Not Agree, then only total deaths are included.</td></tr></tbody>
</table></figure>
<p>Please click <a href='sample_cdc.html'>here</a> to view some data sample of this dataset.</p>
<h3 id='213-data-issues'>2.1.3 Data Issues</h3>
<ul>
<li><p>Different data types such as float and string may appear in the same column.</p>
</li>
<li><p>There are a lot of missing values.</p>
<ul>
<li>For example, we have the value of &quot;tot_case&quot; for a certain city on a certain day, and this value represents the total number of cases. This value should be equal to the sum of the value of &quot;conf_cases&quot; and the value of &quot;prob_cases&quot;. But the last two values in the data set are often blank, even if they should not be 0.
Some data may have a noise value. For example, the value of the number of new infections is -1.</li>

</ul>
</li>

</ul>
<h3 id='214-data-cleanliness'>2.1.4 Data Cleanliness</h3>
<p>For the CDC data set, we mainly made the following assessments：</p>
<ul>
<li>Calculate the fraction of missing values for each attribute.</li>
<li>Calculate the fraction of noise values.</li>

</ul>
<p>The percentage of missing values for each attribute:</p>
<p><img src="https://user-images.githubusercontent.com/35549544/143917182-01b4fa0e-b19f-4cbc-8846-009da273236f.PNG" referrerpolicy="no-referrer" alt="Missing Value"></p>
<p>The fraction of noise values:</p>
<p><img src="https://user-images.githubusercontent.com/35549544/143917244-3216d4da-e829-47ad-92b2-b94c91c5252e.PNG" referrerpolicy="no-referrer" alt="Noise Value"></p>
<h2 id='22-opensky-airline-dataset'>2.2 Opensky Airline dataset</h2>
<h3 id='221-overview'>2.2.1 Overview</h3>
<p>We use python crawler to scrape data from <a href='https://opensky-network.org/'>OpenSky</a> for this dataset. Find a website that contain the data we need, pass the url to the method, then the method will iterate all the href in the html text, add it into the waiting list only if it is the csv file that we need by using &#39;endswith()&#39;. Then use &#39;requests&#39; to scrape filee, and add a downloading percentage bar by using tqdm. Since all scv files are compressed as gz file, we also need to import gzip to decompress the file one by one by script.</p>
<p>In total, the whole dataset contains <strong>15 different columns</strong> and <strong>71645420 records</strong>.</p>
<p>Since it is too large to upload (13.3 GB), we choose to provide a download link for the whole data set.</p>
<h3 id='222-data-sample-and-definition'>2.2.2 Data Sample and Definition</h3>
<figure><table>
<thead>
<tr><th>Column Name</th><th>Description</th></tr></thead>
<tbody><tr><td>callsign</td><td>the identifier of the flight displayed on ATC screens (usually the first three letters are reserved for an airline: AFR for Air France, DLH for Lufthansa, etc.)</td></tr><tr><td>number</td><td>the commercial number of the flight, when available (the matching with the callsign comes from public open API)</td></tr><tr><td>icao24</td><td>the transponder unique identification number;</td></tr><tr><td>registration</td><td>the aircraft tail number (when available);</td></tr><tr><td>typecode</td><td>the aircraft model type (when available);</td></tr><tr><td>origin</td><td>a four letter code for the origin airport of the flight (when available);</td></tr><tr><td>destination</td><td>a four letter code for the destination airport of the flight (when available);</td></tr><tr><td>firstseen</td><td>the UTC timestamp of the first message received by the OpenSky Network;</td></tr><tr><td>lastseen</td><td>the UTC timestamp of the last message received by the OpenSky Network;</td></tr><tr><td>day</td><td>the UTC day of the last message received by the OpenSky Network.</td></tr></tbody>
</table></figure>
<p>Please click <a href='sample_opensky.html'>here</a> to view some data sample of this dataset.</p>
<h3 id='223-data-issues'>2.2.3 Data Issues</h3>
<ul>
<li>There is no noise value in this dataset, only missing value</li>
<li>Missing value for Fields <strong>origin</strong> and <strong>destination</strong>
Origin and destination airports are computed online based on the ADS-B trajectories on approach/takeoff: no crosschecking with external sources of data has been conducted. These two fields are empty when no airport could be found</li>
<li>Missing value for Fields <strong>typecode</strong> and <strong>registration</strong>
Aircraft information come from the OpenSky aircraft database. Fields are empty when the aircraft is not present in the database.</li>
<li>Missing value for Fields <strong>number</strong>
The commercial number of the flight, are empty when unavailable</li>

</ul>
<h3 id='224-data-cleanliness'>2.2.4 Data Cleanliness</h3>
<ul>
<li>The evaluation of the cleaniness is (valid records that we need/ all records that we need), as long as there is a missing value in the record, this record is invalid.</li>
<li>Cleaniness of Sample(First 4000 records of each file) =&gt; 91.8156%</li>
<li>Cleaniness of whole Dataset(the whole dataset is too big to upload - 13.3GB) =&gt; 98.1030%</li>

</ul>
<h2 id='23-scraped-tweets-related-to-covid-and-flight'>2.3 Scraped Tweets related to &#39;Covid&#39; and &#39;flight&#39;</h2>
<h3 id='231-overview'>2.3.1 Overview</h3>
<p>This is a dataset of tweets that is scraped from twitter. We scraped for more than 20 hours to get over 450K tweets in English that contains keywords <code>covid</code> and <code>flight</code> since <code>01-01-2019</code> (It&#39;s an early date but we just want to make sure the data is fully covered. Interesting fact is that the earliest tweet that contains those two words is on <code>02-19-2019</code>, unrelated to covid-19 though).</p>
<p>There are a lot of columns such as datetime, user_id, username, name, tweet, language, mentions, urls, photos, replies_count, retweets_count, likes_count, hashtags, cashtags, link, retweet, quote_url, video, thumbnail, near, geo, etc.</p>
<h3 id='232-data-sample-and-definition'>2.3.2 Data Sample and Definition</h3>
<figure><table>
<thead>
<tr><th>Column Name</th><th>Description</th></tr></thead>
<tbody><tr><td>id</td><td>id of this original tweet</td></tr><tr><td>conversation_id</td><td>id of the conversation that includes this tweet(if applied). Otherwise appears the id of this original tweet</td></tr><tr><td>created_at</td><td>the date and time when this tweet is created (with time zone)</td></tr><tr><td>date</td><td>the date when this tweet is created</td></tr><tr><td>time</td><td>the time when this tweet is created</td></tr><tr><td>timezone</td><td>the timezone of date and time above</td></tr><tr><td>user_id</td><td>id of the user who creates this tweet</td></tr><tr><td>username</td><td>username of the user who creates this tweet</td></tr><tr><td>name</td><td>the nickname of user who creates this tweet</td></tr><tr><td>tweet</td><td>the plain text of tweet</td></tr><tr><td>language</td><td>the language this tweet uses</td></tr><tr><td>mentions</td><td>twitter users that are mentioned in this tweet</td></tr><tr><td>urls</td><td>the urls that appears in this tweet</td></tr><tr><td>photos</td><td>the photos that appears in this tweet</td></tr><tr><td>replies_count</td><td>the number of replies to this tweet</td></tr><tr><td>retweets_count</td><td>the number of retweets of this tweet</td></tr><tr><td>likes_count</td><td>the number of likes of this tweet</td></tr><tr><td>hashtags</td><td>the hashtags that appears in this tweet</td></tr><tr><td>link</td><td>the original link to this tweet</td></tr><tr><td>video</td><td>the number of videos that appear in this tweet</td></tr><tr><td>thumbnail</td><td>the thumbnail of user that creates this tweet</td></tr><tr><td>reply_to</td><td>the people and their id that this tweet replies to(if applied)</td></tr></tbody>
</table></figure>
<p>Please click <a href='sample_twitter.html'>here</a> to view some data sample of this dataset.</p>
<h3 id='233-data-issues'>2.3.3 Data Issues</h3>
<ul>
<li><p>Missing values in many fields.</p>
<ul>
<li>For example, &#39;username&#39;, &#39;mentions&#39;, &#39;retweets&#39;. However, we mainly focus on the tweet column because that is where text data exists.</li>

</ul>
</li>
<li><p>Noise value in &#39;tweet&#39; colunmn. For example, there are lots of emojis and punctuation which are useless for following analysis.</p>
</li>

</ul>
<h3 id='234-data-cleanliness'>2.3.4 Data Cleanliness</h3>
<p>cleaningText.py: # cleaning text</p>
<ul>
<li>using regular expression to filter all the emojis and punctuation</li>
<li>using stopwords to filter all the useless words</li>

</ul>
<pre><code>the percentage of valid content: 19.478%
the percentage of useful words: 78.476%
</code></pre>
<h1 id='3-pre-processing-of-data'>3 Pre-Processing of Data</h1>
<p>We extract <strong><em>8</em></strong> columns from COVID-19 dataset and <strong><em>3</em></strong> columns from flight dataset.</p>
<h2 id='31-covid-19-cases-and-death-dataset'>3.1 Covid-19 Cases and Death Dataset</h2>
<ul>
<li><p>Handling null values</p>
<ul>
<li>For null values, we decided to fill them with mean values. Because we need to make sure there is a relation about the columns, such as tot_cases = conf_cases + prob_cases.</li>

</ul>
</li>

</ul>
<pre><code class='language-python' lang='python'># handle null value of each column
data[col].fillna(mean, inplace = True)
</code></pre>
<ul>
<li>Checking the duplicates</li>

</ul>
<pre><code class='language-python' lang='python'># check duplicate
print(df[df.duplicated()])
</code></pre>
<h2 id='32-opensky-airline-dataset'>3.2 Opensky Airline dataset</h2>
<ul>
<li>Since we only need the origin, destination, icao24 and date of flights for the current project, delete any records that contain nan values or empty values in above four columns.</li>
<li>Also, for this project, we only need flights that take place in the United States, so we use python crawler to get the aiports and their corresponding states from wiki, so we can identify and delete records that containing airports that are not in the United States.</li>

</ul>
<h2 id='33-scraped-tweets-related-to-covid-and-flight'>3.3 Scraped Tweets related to &#39;Covid&#39; and &#39;flight&#39;</h2>
<p>In this part, we need to decide what content of a tweet is not clean. This decision should also made under the consideration of what we need to do with those tweets. After our discussion, we decided to do Sentiment analysis with all these tweets (will be further described in that section).</p>
<p>After reading quite a few scraped tweets, we found out that these 4 kinds of noise should be cleaned.</p>
<ul>
<li>Stopwords</li>
<li>URLs (<a href='https://xxx.dd.com/dasdfa' target='_blank' class='url'>https://xxx.dd.com/dasdfa</a>)</li>
<li>Hashtags (#sometag)</li>
<li>Mentioned tags (@somebody)</li>

</ul>
<p>The reason for removing stopwords is quite clear, there&#39;s no need analyze the sentiment of stopwords(which usually don&#39;t have emotional meaning afterall). This reason also applies for URLs and Mentioned tag. Whether to remove hashtags is discussed more. We decided to remove it simply because, yes it do have meaning sometimes, but most of the time they only represent some certain objective things(it&#39;s a noun mostly) which don&#39;t tend to have emotional meaning.</p>
<p>We used <code>nltk</code> package to tokenize tweets. We also used it as our stopwords list. In terms of urls and hash tags and mentioned tags, we came up with two regular expressions to find and replace them.</p>
<p>Every tweet is read from csv file named <code>result_covid_flight.csv</code> and then rewrite into a new csv file named <code>result_covid_flight_cleaning.csv</code>. As we collected 470K records and there&#39;s no way to upload it on github, we created a share link on google drive for you to use.</p>
<pre><code>https://drive.google.com/file/d/1OA1kkjQ9V0ZXQ-s8MqbLsOXATgK4k-IW/view
</code></pre>
<h1 id='4-analysis'>4 Analysis</h1>
<h2 id='41-basic-statistical-analysis'>4.1 Basic Statistical Analysis</h2>
<h3 id='411-mean-mode-median'>4.1.1 Mean, Mode, Median</h3>
<h4 id='4111-opensky-airline-dataset'>4.1.1.1 Opensky Airline dataset</h4>
<p>The three columns data we got here are origin airports, destination airports and the date of flight. Where the mean and standard divation of those three variables is meaningless, so we choose to calculate the median and mode (Actually the median is also meaningless).
Average flights per day =&gt; 10491
Here is the hitogram for flights per day:</p>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/lyt/figures/opensky_histogram.html'><strong>plotly 链接</strong></a></p>
<h4 id='4112-cdc-dataset'>4.1.1.2 CDC dataset</h4>
<p>Here are mean, median, std values of variables in COVID-19 dataset.</p>
<p><a href='www.google.com'>**plotly link</a></p>
<h3 id='412-anomalies-detection'>4.1.2 Anomalies Detection</h3>
<h4 id='4121-opensky-airline-dataset'>4.1.2.1 Opensky Airline dataset</h4>
<p>As for the anomalies detection. Since this data could be considered as kind of time series, we choose to calculate the anomalies point by using S-H-ESD method. We constructed a variable named flights, which is used for represent the number of flights that launched that day, which is also used for anomalies detection. For those points, we choose to keep them since they are part of our data, which is the real data, could got some unexpected situation, and that&#39;s what we are going to dig.</p>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/lyt/figures/opensky_ourliers.html'><strong>plotly 链接</strong></a></p>
<h4 id='4122-cdc-dataset'>4.1.2.2 CDC dataset</h4>
<p>Using <strong><em>lof</em></strong> to detect anomlies on CDC dataset</p>
<p>the inliers in the output are the new variables that do not contain the outliers.
First compute the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.
Use method as a threshold value for distinguishing outliers and common points.</p>
<p>Here are three different values of K to detect anomlies</p>
<p>K = 5:</p>
<p><img src="figures/lofk=5.png" referrerpolicy="no-referrer"></p>
<p>K = 10:</p>
<p><img src="figures/lofk=10.png" referrerpolicy="no-referrer"></p>
<p>K = 15</p>
<p><img src="figures/lofk=15.png" referrerpolicy="no-referrer"></p>
<h5 id='41221-how-s-h-esd-work'>4.1.2.2.1 How S-H-ESD Work</h5>
<p><img src="figures/S-H-ESD.png" referrerpolicy="no-referrer"></p>
<h2 id='42-bin-the-data'>4.2 Bin the data</h2>
<h3 id='421-covid-19-cases-and-death-dataset'>4.2.1 Covid-19 Cases and Death Dataset</h3>
<p>Here is the overvew of the <strong>binned</strong> data.</p>
<p>We chose to bin the data that combined by two columns - &#39;origin&#39; and &#39;destination&#39;. Which is actually the number of flights per day after the processing. As for the binning strategy we used in this part, is Equal-width binning, which is pretty good for those kind of data. Since the number of flights would be pretty stable if there is no influence.</p>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/lyt/figures/opensky_bin.html'><strong>plotly 链接</strong></a></p>
<p>From this chart we can know that average flights per day is within 6906 to 10358, and the overall trend is toward higher frequencies. In fact, from here we can come up with a more superficial hypothesis that people&#39;s travel and their attitudes towards the new cap and the spread of the disease have no direct relationship</p>
<h3 id='422-covid-19-cases-and-death-dataset'>4.2.2 Covid-19 Cases and Death Dataset</h3>
<ul>
<li>The tot_cases is the key data in the CDC data, reflecting the number of patients in a certain state on a certain day. Through the binning operation, we can get the distribution of the number of patients, we can <strong>eliminate some abnormal data</strong>, <strong>realize the discretization of the data</strong>, <strong>improve the robustness of the data</strong>, and will not collapse due to the input of some extreme size data. In addition, if we further subdivide into each state, we can get the peak date of the epidemic in each state.</li>
<li>For the tot_cases, we divide it into intervals with <strong>same width</strong> .</li>
<li>We can find that the first bin concludes half of the cases.</li>

</ul>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/ym/binning.html'>binning.html</a></p>
<h2 id='43-histograms-and-correlations'>4.3 Histograms and Correlations</h2>
<h3 id='431-histograms'>4.3.1 Histograms</h3>
<ul>
<li>For CDC dataset, we made histograms of tot_cases, conf_cases and prob_cases.</li>
<li>The abscissa represents the interval of the data box, and the ordinate represents the number of people.</li>
<li>We can see that the histogram shapes of tot_cases and conf_cases are very similar. And the frequency range also crystallizes. Since tot_cases is obtained by adding conf_cases and prob_cases, we can conclude that the size of tot_cases is mainly determined by the size of conf_cases.</li>

</ul>
<h4 id='1-historgram-for-totcases'>1. historgram for tot_cases</h4>
<p><img src="https://user-images.githubusercontent.com/35549544/138144242-7733ef2d-60c1-4770-9425-7f0306fa780d.png" referrerpolicy="no-referrer"></p>
<h4 id='2-historgram-for-confcases'>2. historgram for conf_cases</h4>
<p><img src="https://user-images.githubusercontent.com/35549544/138144262-943465aa-cde7-4158-86c9-876b4159d84b.png" referrerpolicy="no-referrer" alt="image"></p>
<h4 id='3-historgram-for-probcases'>3. historgram for prob_cases</h4>
<p><img src="https://user-images.githubusercontent.com/35549544/138144276-1c0628ee-67e0-4963-b909-4ef6607c9b45.png" referrerpolicy="no-referrer" alt="image"></p>
<h3 id='432-correlations'>4.3.2 Correlations</h3>
<h4 id='4321-correlation-between-totcases-and-totdeath'>4.3.2.1 correlation between tot_cases and tot_death</h4>
<ul>
<li>In this picture, We can see that the scattered points formed by tot_cases and tot_death present <strong>multiple curves shapes</strong>. And by observing these shapes, we can conclude that the overall relationship between these two data is <strong>proportional</strong>.</li>
<li>In addition, if we observe the scatter plot in the figure, we can find that the scatter points are not completely concentrated together to form a sphere, but form multiple curves, starting from (0, 0). Such a result may reflect the relationship between the total number of diagnoses and the total number of deaths in different <strong>states</strong>, or it may be the influence of the <strong>season</strong>, which makes the image present such characteristics.</li>

</ul>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/ym/corrleation%20totcases%26totdeath.html'>图片 1</a></p>
<h4 id='4322-correlation-between-totcases-and-probdeath'>4.3.2.2 correlation between tot_cases and prob_death</h4>
<ul>
<li>In this picture we can see that the scattered points also form many curves that are close to straight lines, instead of clustering into a cluster. The analysis of this situation is the same as the analysis in the first picture.</li>
<li>What is worth noting is the scattered point set of <strong>two approximate horizontal lines</strong> in the figure. The following one reflects the situation that prob_death is 0 but tot_cases is not 0 happens many times. The above item reflects that when prob_death occurs with a high frequency between 0 and 1000, this situation is caused by filling the blank part of prob_death with the average value.</li>
<li>The resaon for this method is： If we do not fill in, we will lose nearly <strong>half of the data</strong> , because it is time series data, if the data loses a long period of time, it will be difficult to compare the data characteristics of different time periods. For example, the influence of the season on the epidemic situation in a certain state.</li>

</ul>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/ym/corrleation%20prob%20death%20%26%20tot%20cases.html'>图片 2</a></p>
<h4 id='4323-correlation-between-totdeath-and-prob-death'>4.3.2.3 correlation between tot_death and prob death</h4>
<ul>
<li>In this picture, we can find that the relationship diagram between pro_death and tot_death is very close to the relationship diagram between pro_death and tot_cases. Since tot_death and tot_cases are close to a linear relationship, such a result can also be inferred.</li>

</ul>
<p><a href='https://github.com/singh-classes/project-4-segmentfault/blob/ym/corrleation%20prob%20death%20%26%20tot%20death.html'>图片 3</a></p>
<h2 id='44-cluster-analysis'>4.4 Cluster Analysis</h2>
<ul>
<li>For each dataset, we only display one figures, for more, please refer to <a href='https://github.com/singh-classes/project-2-segmentfault/tree/main/pics/'>Project2</a></li>

</ul>
<h3 id='441-opensky'>4.4.1 opensky</h3>
<p>We have conducted all four cluster analyses on the opensky dataset. However, since all numeric dataset we got are time series, it can&#39;t do cluster by above 4 method. So we do combine several columns into two new columns named &#39;flights per day&#39; and &#39;flights happened in Mode airports per day&#39;. But nothing changed, even though we have made out four of them, they seems like meaningless. The only only thing we can tell form the figures is that the &#39;flights per day&#39; and &#39;flights happened in Mode airports per day&#39; do key the same correlation at the most the time.</p>
<ul>
<li><img src="figures/opensky_cluster_kmean.png" referrerpolicy="no-referrer"></li>

</ul>
<h3 id='442-cdc'>4.4.2 CDC</h3>
<p>We have conducted all four cluster analyses on the opensky dataset. However, since all numeric dataset we got are time series, it can&#39;t do cluster by above 4 method. So we do combine several columns into two new columns named &#39;new death per day&#39; and &#39;new cases per day&#39;. But nothing changed, even though we have made out four of them, they seems like meaningless. The only only thing we can tell form the figures is that the &#39;new deatch per day&#39; and &#39;new cases per day&#39; do key the same correlation at the most the time.</p>
<ul>
<li><img src="figures/CDC_cluster_kmean.png" referrerpolicy="no-referrer"></li>

</ul>
<h2 id='45-sentiment-analysis'>4.5 Sentiment Analysis</h2>
<p>Sentiment Analysis via Scraped Tweets.</p>
<p>We chose Sentiment Analysis because of two reasons.</p>
<ol start='' >
<li>Tweets are too short to use topic modeling. In Professor Lisa&#39;s lecture, we saw how bad it cloud become, there won&#39;t be any good result of topics to show. More importantly, we don&#39;t want the topic of it! These data are collected because they are related to <code>covid-19</code> and <code>flight</code>. That&#39;s already our topics!</li>
<li>We do want to analyze how people feel about <code>covid-19</code> when <code>travel through air</code>. Perhaps sentiment analysis won&#39;t tell exactly how but that&#39;s the way on it.</li>

</ol>
<p>To be more specific, why we want to find sentiment of tweets? First, we want to know how people feel when they are traveling by plane under the pandemic of Covid-19, that is, does people feel positive more or negative more under such circumstances. Second, we want to know the sentiment through out the timelines. When does people feel negative more?</p>
<h3 id='451-sentiment-grading'>4.5.1 Sentiment Grading</h3>
<p>We use <code>vaderSentiment</code> package to do grading. This is a great package based on lexicon method. It does not only contains emotional vocabulary, but also has a lot <code>emoji</code> and <code>emoticon</code>. We use it to calculate the sentiment for each tweet.</p>
<p>The result for this part is also too large to upload. It extract the content in <code>result_covid_flight_cleaning.csv</code> (which is also generated in <code>Tweets cleaning</code> section) and generate results in a new csv file named <code>result_covid_flight_cleaning_sentiment.csv</code>.</p>
<p>For more details please check it in our code file <code>TweetsSentiment.py</code></p>
<h3 id='452-sentiment-labeling-and-accuracy'>4.5.2 Sentiment Labeling and Accuracy</h3>
<p>For this part we manually add tags by determining the sentiment for a tweets. It included 2 parts here.</p>
<p>First we need to sample a few tweets. Here we extracted 50 tweets randomly from <code>result_covid_flight_cleaning.csv</code>. And we manually labeled each of these 50 tweets. Sometimes it&#39;s really hard to determine it is positive, negative or neutral even for human beings like us. We wrote a small script named <code>TweetsTagging.py</code> for this task and it will generate a tagged file named <code>result_covid_flight_cleaning_tag.csv</code> (and it is uploaded on github). <strong>Please don&#39;t run it</strong> unless you want to overwrite our tagging result and re-tag it again.</p>
<p>Second we want to compare it to the sentiment grading done by <code>vaderSentiment</code>. In the code <code>TweetsSentimentAccuracy.py</code>, we calculated the sentiment of each tweet and compared it with our manually labels. It will generate the result in a file named <code>TweetsSentimentAccuracyResult.txt</code>.</p>
<pre><code>Positive tweets accuracy: 4/4
Negative tweets accuracy: 10/27
Neutral tweets accuracy: 8/19
Total tweets accuracy: 22/50 = 0.44
</code></pre>
<h3 id='453-sentiment-analysis'>4.5.3 Sentiment Analysis</h3>
<p>For this part we want to analyze some result by grading sentiment for each tweets. We summarized the total amount of tweets in each month by their sentiment. The result (<code>TweetsSentimentAnalysisResult.txt</code>) is generated by code file named <code>TweetsSentimentAnalysis.py</code>and it looks like this:</p>
<pre><code>month       pos   neg   neu
2021-08    [7759, 8106, 3996]
2021-07    [6802, 6988, 2922]
2021-06    [6785, 6070, 3391]
2021-05    [7937, 8557, 3939]
2021-04    [9452, 11601, 5548]
2021-03    [6045, 6331, 2726]
2021-02    [6452, 6457, 3136]
2021-01    [10841, 12724, 5194]
2020-12    [12646, 19225, 6153]
2020-11    [9028, 7321, 4133]
2020-10    [9842, 10077, 3912]
2020-09    [8888, 8893, 4255]
2020-08    [10677, 9769, 5160]
2020-07    [12560, 11426, 5593]
2020-06    [11901, 10048, 4416]
2020-05    [18170, 10801, 6237]
2020-04    [18425, 12129, 5403]
2020-03    [16272, 17767, 7922]
2020-02    [1040, 1138, 537]
2020-01    [1, 0, 0]
2019-06    [0, 0, 1]
</code></pre>
<pre><code>month       pos     neg     neu
2021-08    [39.07%, 40.81%, 20.12%]
2021-07    [40.70%, 41.81%, 17.48%]
2021-06    [41.76%, 37.36%, 20.87%]
2021-05    [38.84%, 41.88%, 19.28%]
2021-04    [35.53%, 43.61%, 20.86%]
2021-03    [40.03%, 41.92%, 18.05%]
2021-02    [40.21%, 40.24%, 19.55%]
2021-01    [37.70%, 44.24%, 18.06%]
2020-12    [33.26%, 50.56%, 16.18%]
2020-11    [44.08%, 35.74%, 20.18%]
2020-10    [41.30%, 42.29%, 16.42%]
2020-09    [40.33%, 40.36%, 19.31%]
2020-08    [41.70%, 38.15%, 20.15%]
2020-07    [42.46%, 38.63%, 18.91%]
2020-06    [45.14%, 38.11%, 16.75%]
2020-05    [51.61%, 30.68%, 17.71%]
2020-04    [51.24%, 33.73%, 15.03%]
2020-03    [38.78%, 42.34%, 18.88%]
2020-02    [38.31%, 41.92%, 19.78%]
2020-01    [100.00%, 0.00%, 0.00%]
2019-06    [0.00%, 0.00%, 100.00%]
</code></pre>
<p>Judging by the result we may conclude that:</p>
<ul>
<li>Positive tweets are apparently more than negative tweets in the beginning months that COVID-19 pandemic just started. People using English on twitter are significantly confident overall. We guess that most of them at that time believes that it will all be gone soon.</li>
<li>As time went by, tweets from people tends to be balanced between positive sentiment and negative sentiments. There are some months that negative sentiments are significantly more than positive sentiments.</li>
<li>People talked it more when COVID-19 just began. The tendency of which tends to be lower as time goes by.</li>

</ul>
<h2 id='46-hypothesis'>4.6 Hypothesis</h2>
<blockquote><p>我也没想清楚这个到底是放在 analysis 里还是单独拿出来讲，我感觉好像都可以，单独拿出来那就是一个独立的 tab 了，直接照搬 project3 的内容</p>
</blockquote>
<h3 id='461-hypothesis1-decision-tree'>4.6.1 Hypothesis1 (Decision Tree)</h3>
<p><em>Although the number of flights per day is generally on the rise. But people&#39;s willingness to travel is still affected by the number of new cases and deaths each day.</em></p>
<p>We used decision tree to test since we want to prove that there is a relationship between the daily new cases and deaths and people&#39;s willingness to take plane. The hypothesis assumes that the daily new cases and new deaths would influence people&#39;s willingness, but we don&#39;t know how it works, so we choose to use decision tree classifier here.
(With both opensky and CDC data set).</p>
<p>We preprocess the data by reducing the outliers, fill the missing value with the mean or median. And since both data set is time series, we have to make sure the data we used for this hypothesis test should in the same time range. So we choose the data based on the CDC data set. When there are both records for today and tomorrow in opensky data sets, choose this CDC records and store the group id.</p>
<p>As for how we divided those data into groups</p>
<ul>
<li>When the flights number is decrease by tomorrow, mark it as &#39;-1&#39;,</li>
<li>When flights number is increase by tomorrow, mark it as &#39;1&#39; (If remain the same, mark it as &#39;0&#39;, which is impossible in our data set, so there are two groups in total.)</li>

</ul>
<p>Here is the reports generated by the provided methods:</p>
<p><img src="figures/opensky_CDC_decisionTree.png" referrerpolicy="no-referrer"></p>
<p>As we can see here, the sorce generated by those method tells us there do have a relation between people&#39;s mind about taking flights and daily new cases and deaths. However, those outputs also donate that the relation is so weak that only a small part of people would consider about the daily new cases and deaths when they are going to take plane.</p>
<p>So, our data do support this hypothesis, but the relationship between them is not completely assured.</p>
<h3 id='462-hypothesis-2-t-test'>4.6.2 Hypothesis 2 (T-Test)</h3>
<p>As for t-test, we need to set a null hypothesis and set the significance standard alpha which will measure if the result meets the hypothesis or not.</p>
<p>Then we need to calculate the statistical values t(which measures if the average of a sample is quite different from the average of all) and we can get p_value by looking up in the table to compare p_value and the significance standard alpha</p>
<p><strong><em>The average conf_cases in all states are 30,000.</em></strong></p>
<p>This hypothesis mainly focus on the difference of the number of confirmed COVID-19 cases between states in America, because people have to know if different states have different panic situation to make sure if it is safe to take flights.</p>
<p>here is the result (p_value &lt; significance alpha)
<img src="figures/ttest.png" referrerpolicy="no-referrer"></p>
<h3 id='463-hypothesis-3-anova-test'>4.6.3 Hypothesis 3 (Anova test)</h3>
<p>As for anova test, we need to set a null hypothesis and set the significance standard alpha which will measure if the result meets the hypothesis or not.</p>
<p>Then we need to calculate the statistical values ssb(difference between groups) and ssw(difference within group) and we can get f_value by calling statistic function to compare f_value and the significance standard alpha</p>
<p><strong><em>There is no significant difference between covid-19 cases(including conf_cases, prob_cases).</em></strong></p>
<p>This hypothesis mainly focus on the difference between confirmed COVID-19 cases and probable COVID-19 cases in America.</p>
<p>here is the result (f_value &lt; significance alpha)
<img src="figures/anova.png" referrerpolicy="no-referrer"></p>
<h2 id='47-classification'>4.7 Classification</h2>
<p><strong>There are some more information that wasn&#39;t shown directly on data. For example, death rate in CDC Covid-19 dataset. In this part we labeled death rate on CDC dataset and tried all 6 classifiers to classify data by death rate, in order to figure out how well they each did on this classification task.</strong></p>
<p>We evaluate their performances by <code>accuracy_score</code>, <code>classification report</code>(include <code>precision</code>, <code>f1-score</code>, <code>macro avg</code>, <code>weighted avg</code> for each class.)</p>
<p>We also plot <code>ROC curve</code> and <code>aur_score</code> for each classifier. As we know <code>sk-learn</code> only supports <code>ROC curve</code> for binary classifier. It&#39;s typical to draw a <code>ROC curve</code> by each class. So we will transform the result data into a binary one for each class, and then draw them on the plot by class. Check them down below.</p>
<h3 id='471-how-is-data-pre-processed-and-split'>4.7.1 How is data pre-processed and split</h3>
<p>We first dealt with the missing and error in the data based on the analysis of Proj2. After that, we use tot_death and tot_cases in the processed data to divide and calculate the corresponding death_rate. This variable can be understood as the proportion of deaths in all cases.</p>
<p>Next, according to the value of death_rate, we perform labeling for later classification. Mark the top 25% part of death_rate in the data as type &#39;2&#39;, which represents high mortality; mark the 25%~75% part of death_rate in the data as type &#39;1&#39;, which represents medium mortality; mark death_rate in the data The last 25% of the part is marked as type &#39;0&#39;, which represents low mortality.</p>
<p>The figure below shows the Data summary, including the 25% point and 75% of death rate:</p>
<p><img src="figures/Data summary.png" referrerpolicy="no-referrer"></p>
<h3 id='472-logistical-regression'>4.7.2 Logistical Regression</h3>
<p>Here is the accuracy rate, confusion matrix and reports generated by Logistical Regression:</p>
<p><img src="figures/LR result.png" referrerpolicy="no-referrer"></p>
<p>The ROC curve is:</p>
<p><img src="figures/LR ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='473-decision-tree'>4.7.3 Decision Tree</h3>
<p>Here is the accuracy rate, confusion matrix and reports generated by Decision Tree:</p>
<p><img src="figures/Decision Tree result.png" referrerpolicy="no-referrer"></p>
<p>The ROC curve is:</p>
<p><img src="figures/DT ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='474-knn'>4.7.4 kNN</h3>
<p>Here is the accuracy rate, confusion matrix and reports generated by kNN:</p>
<p><img src="figures/KNN result.png" referrerpolicy="no-referrer"></p>
<p>The ROC curve is:</p>
<p><img src="figures/kNN ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='475-naive-bayes'>4.7.5 Naive Bayes</h3>
<p>Here is the accuracy rate, confusion matrix and reports generated by Naive Bayes:</p>
<p><img src="figures/NB result.png" referrerpolicy="no-referrer"></p>
<p>The ROC curve is:</p>
<p><img src="figures/NB ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='476-svm'>4.7.6 SVM</h3>
<p>For the classification task done by SVM, well the result wasn&#39;t satisfied and it took more than the sum of all other 5 methods to finish. We used our training dataset to train this model and use validation dataset to test its performance. I tried all 3 SVM models, which are <code>svc</code>, <code>NuSVC</code> and <code>LinearSVC</code>. <code>SVC</code> did best so the result is performed by <code>svc</code> here.</p>
<p>We also used cross validation to train this model, still not so good.</p>
<p><img src="figures/SVM result.png" referrerpolicy="no-referrer"></p>
<p>The ROC curve is:</p>
<p><img src="figures/SVM ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='477-random-forest'>4.7.7 Random Forest</h3>
<p>For the classification task done by random forest, it did really good. We used our training dataset to train this model and use validation dataset to test its performance.</p>
<p><img src="figures/random forest result.png" referrerpolicy="no-referrer"></p>
<p>The <code>ROC curve</code> for random forest is also quite good-looking. It shows strong relation for each type.</p>
<p><img src="figures/random forest ROC.png" referrerpolicy="no-referrer"></p>
<h3 id='478-overall-comparison'>4.7.8 Overall comparison</h3>
<p>Here is the comparison of all six algorithm:</p>
<p><img src="figures/Algorithm Comparsion.png" referrerpolicy="no-referrer"></p>
<p>[可交互图表]<a href='https://github.com/singh-classes/project-4-segmentfault/blob/ym/alg%20accuray%20score.html' target='_blank' class='url'>https://github.com/singh-classes/project-4-segmentfault/blob/ym/alg%20accuray%20score.html</a></p>
<p>Note that our labels are formed by calculation of a few columns. So from all these 6 classification accuracy result we can learn that:</p>
<ul>
<li><code>Random Forest</code> and <code>Decision Tree</code> did best in our classification task</li>
<li><code>Naive Bayes</code> did worst in our classification task</li>
<li>It implies that <code>Naive Bayes</code> might not detect much division relationship, while <code>Random Forest</code> and <code>Decision Tree</code> does.</li>
<li><code>kNN</code> is in the upper middle.</li>
<li><code>SVM</code> and <code>Logistical Regression</code> are not quite good as the best. But they did not too bad. However they cost most of the time.</li>

</ul>
<h1 id='48-additional-cluster-analysis'>4.8 Additional Cluster Analysis</h1>
<h2 id='481-opensky'>4.8.1 opensky</h2>
<p>We have conducted all four cluster analyses on the opensky dataset. However, since all numeric dataset we got are time series, it can&#39;t do cluster by above 4 method. So we do combine several columns into two new columns named &#39;flights per day&#39; and &#39;flights happened in Mode airports per day&#39;. But nothing changed, even though we have made out four of them, they seems like meaningless. The only only thing we can tell form the figures is that the &#39;flights per day&#39; and &#39;flights happened in Mode airports per day&#39; do key the same correlation at the most the time.
<img src="figures/opensky_cluster_dbscan.png" referrerpolicy="no-referrer">
<img src="figures/opensky_cluster_kmean.png" referrerpolicy="no-referrer">
<img src="figures/opensky_cluster_gmm.png" referrerpolicy="no-referrer">
<img src="figures/opensky_cluster_ward.png" referrerpolicy="no-referrer"></p>
<h2 id='482-cdc'>4.8.2 CDC</h2>
<p>We have conducted all four cluster analyses on the opensky dataset. However, since all numeric dataset we got are time series, it can&#39;t do cluster by above 4 method. So we do combine several columns into two new columns named &#39;new death per day&#39; and &#39;new cases per day&#39;. But nothing changed, even though we have made out four of them, they seems like meaningless. The only only thing we can tell form the figures is that the &#39;new deatch per day&#39; and &#39;new cases per day&#39; do key the same correlation at the most the time.
<img src="figures/CDC_cluster_dbscan.png" referrerpolicy="no-referrer">
<img src="figures/CDC_cluster_kmean.png" referrerpolicy="no-referrer">
<img src="figures/CDC_cluster_gmm.png" referrerpolicy="no-referrer">
<img src="figures/CDC_cluster_ward.png" referrerpolicy="no-referrer"></p>
<h1 id='5-conclusion'>5 Conclusion</h1>
<p>There is no doubt that travels by air is affected by COVID-19.</p>
<h2 id='51-peoples-moods-are-affected-by-covid-19'>5.1 People’s moods are affected by COVID-19</h2>
<ul>
<li>People are more positive in terms of travel by air at the beginning of this pandemic.</li>
<li>People become more negative as time went by.</li>

</ul>
<p>Through sentiment analysis we know that sentiment of people are unstable. They tends to be more positive at first. They might be confident and thought COVID would all ends really soon. And then they become more negative cause it didn&#39;t end at all.</p>
<h2 id='52-only-a-part-of-travelers-are-finally-stopped-by-covid-19'>5.2 Only a part of travelers are finally stopped by COVID-19</h2>
<ul>
<li>Travelers are stopped most at the beginning of this pandemic.</li>
<li>Travelers made up their minds as time went by.</li>

</ul>
<p>By combining flights dataset with CDC dataset we know that COVID-19 did stopped people traveling, but not significantly. They are mostly stopped at the beginning of this pandemic, and as time went by, people who decided to travel by air still travel.</p>
<h1 id='6-ethical-considerations--limitations--future-work'>6 Ethical considerations &amp; Limitations &amp; Future Work</h1>
<p>A few ethical considerations and limitations in our project, and how can we do better in the future.</p>
<h2 id='61-bias'>6.1 Bias</h2>
<h3 id='611-only-airlines-data-included'>6.1.1 Only airlines data included</h3>
<p>The first thing is bias. Apparently the fact that we only discussed about airlines made our bias.</p>
<p>We didn&#39;t take other forms of transportation into consideration mainly because they are hard to obtain.</p>
<p>But it&#39;s still a fact that our project can do better if we can compare airlines data with trains records or some other forms.</p>
<h3 id='612-only-us-regions-are-considered'>6.1.2 Only US regions are considered</h3>
<p>Another bias in our project is that we only considered US regions.</p>
<p>If we can get flights data and covid-19 cases datasets all around the world, we can then do comparison to find out to what degree covid has affect each countries.</p>
<h2 id='62-tweets'>6.2 Tweets</h2>
<h3 id='621-related-to-specific-user'>6.2.1 Related to specific user</h3>
<p>Another ethical consideration is about our dataset of tweets.</p>
<p>Even though it is scraped through publicly available, but it is related to specific twitter user. And this data will remains in our dataset even if that user deletes it.</p>
<h3 id='622-data-remains'>6.2.2 Data remains</h3>
<p>Tweet data will remains in our dataset even if that user deletes it.</p>
<h2 id='63-sentiment-analysis'>6.3 Sentiment Analysis</h2>
<h3 id='631-not-accurate-enough'>6.3.1 Not accurate enough</h3>
<p>As for sentiment analysis, we did come up with a few conclusions by that, but the sentiments within tweets are not accurate enough.</p>
<p>It is too unclear that we can&#39;t figure our their thought correctly, result in we cannot rely on those sentimental analysis. So what we are going to work on would be improving the accuracy of our sentimental analysis. Thus making our sentiment analysis more convincing.</p>
<h2 id='64-data-combinations'>6.4 Data Combinations</h2>
<h3 id='641-text-data-should-be-somehow-considered-into-analysis'>6.4.1 Text data should be somehow considered into analysis</h3>
<p>Text data are solely analyzed. We didn&#39;t combine text data with other datasets when we are doing data analysis. We should definitely try to vectorize these text data and combine them with other forms of data and analyze them together.</p>
<h3 id='642-network-graph-isnt-combined-with-text-data'>6.4.2 Network graph isn’t combined with text data</h3>
<p>We did draw the network graph for opensky dataset, which should generate many useful information about the correlations between pandemic and people&#39;s mind about it.</p>
<p>However, since we got few datasets to compare with, we didn&#39;t get any useful information, which shouldn&#39;t be. So the network analysis should also be a big part of our future work.</p>
<p>[可交互图表]<a href='https://github.com/singh-classes/project-4-segmentfault/blob/lyt/figures/opensky_network.html' target='_blank' class='url'>https://github.com/singh-classes/project-4-segmentfault/blob/lyt/figures/opensky_network.html</a></p>
</body>
</html>